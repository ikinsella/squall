# What is Squall? 

Squall is a web application whose primary purpose is to facilitate use of the [HTCondor](https://research.cs.wisc.edu/htcondor/) job scheduler for workflows consisting of parameter sweeps by easing the deployment of large batches of jobs and aggregating the output of these jobs in a database for analysis. The development of Squall was inspired by challenges faced by multi-variate pattern analysis research at the University of Wisconsin-Madison, but it can be applied by researchers in any domain employing similarly structured computational workflows (Biomedical Informatics, Optimization, ect.). 

The web application consists of a number of forms used to collect the information (code, data, parameters, etc.) required to deploy computation on [HTCondor](https://research.cs.wisc.edu/htcondor/) execute nodes and metadata to organize the results of this computation. Squall uses this information to generate all of the dags, submission files, wrapper scripts, and pre/post scripts required deploy a batch of jobs using [HTCondor](https://research.cs.wisc.edu/htcondor/). The results of a a single batch of jobs are aggregated into a single [JSON](https://en.wikipedia.org/wiki/JSON) file which can be returned to Squall for organization and storage. Results and meta-data are collected in [MongoDB](https://www.mongodb.org/) (a document based database) so that analysis can be performed on the results produced by any number of jobs by querying any specific attribute by which the results are organized (algorithm used to run the job, data used to perform the computation, specific parameter values, metrics produced in the results, metadata, ect.). 

In short, Squall is an:
 * Efficiency tool to automate the deployment parameter sweeps on [HTCondor](https://research.cs.wisc.edu/htcondor/)
 * Organizational tool to track provenance of results generated from these sweeps
 * Analysis tool to facilitate the access and results across many sweeps

![](https://github.com/ikinsella/squall/blob/master/images/Workflow.png)

The above image depicts a typical workflow using Squall to deploy jobs and collect results. First a user would use the Squall web application to submit information (code, data, etc.) for the jobs they would like to deploy. This information will be used to generate a .zip file containing a directory structure with all of the [DAGs](http://research.cs.wisc.edu/htcondor/manual/v7.8/2_10DAGMan_Applications.html), submit files, wrapper scripts, and pre/post scripts to deploy the batch of jobs. The user transports this zip file to the submit node using a [File Transfer Protocol (FTP)](https://en.wikipedia.org/wiki/File_Transfer_Protocol) ([SCP](https://en.wikipedia.org/wiki/Secure_copy), [FileZilla](https://filezilla-project.org/), ect.). The user unzips the directory structure and [submits](http://research.cs.wisc.edu/htcondor/manual/current/condor_submit_dag.html) the top level [DAG](http://research.cs.wisc.edu/htcondor/manual/v7.8/2_10DAGMan_Applications.html) (sweep.dag). At this point, the jobs will be deployed across available execute nodes on [HTCondor](https://research.cs.wisc.edu/htcondor/) and the results will be aggregated into a single [JSON](https://en.wikipedia.org/wiki/JSON) structure upon completion. Finally, the user returns to the submit node to collect their results and submits the [JSON](https://en.wikipedia.org/wiki/JSON) file (results.json) to the web application. For a more detailed explanation of this process, please see our [usage instructions](https://github.com/ikinsella/squall/wiki/Usage-Instructions) page. 

For more information on the [HTCondor](https://research.cs.wisc.edu/htcondor/) job scheduler and obtaining access to [High Throughput Computing](https://en.wikipedia.org/wiki/High-throughput_computing) resources, please see the [Center for High Throughput Computing](http://chtc.cs.wisc.edu/) (CHTC) and [Open Science Grid](http://www.opensciencegrid.org/) (OSG) websites.  Also see the [Squall wiki](https://github.com/ikinsella/squall/wiki) for more information on this project.
